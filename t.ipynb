{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0900ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Clutch.co crawler (reviews + social links) with anti-bot mitigation.\n",
    "\n",
    "- curl_cffi Session + HTTP/2\n",
    "- Rotate impersonate (chrome141 -> chrome120 -> safari17_0)\n",
    "- Retry with exponential backoff + jitter on 403/429\n",
    "- Random sleep between requests\n",
    "- Graceful skip when blocked\n",
    "- CLI: --limit, --out, --start-url\n",
    "\n",
    "Usage:\n",
    "    python crawl_clutch.py --limit 20 --out clutch_reviews.xlsx\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional, List\n",
    "from urllib.parse import urljoin\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import shutil, random, tempfile, json\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:  # pragma: no cover\n",
    "    # Fallback khi kh√¥ng c√†i tqdm\n",
    "    def tqdm(x, **kwargs):\n",
    "        return x\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from curl_cffi import requests as creq\n",
    "import fcntl\n",
    "import shutil, os\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config m·∫∑c ƒë·ªãnh\n",
    "# =========================\n",
    "DEFAULT_START_URL = \"https://clutch.co/it-services\"\n",
    "\n",
    "# Kh√¥ng t·ª± set User-Agent n·∫øu d√πng impersonate\n",
    "COMMON_HEADERS = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://clutch.co/\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "# Th·ª© t·ª± th·ª≠ impersonate khi b·ªã ch·∫∑n\n",
    "IMP_CHOICES = [\"chrome141\", \"chrome120\", \"safari17_0\"]\n",
    "\n",
    "# Kho·∫£ng ng·ªß ng·∫´u nhi√™n gi·ªØa c√°c request (gi√¢y)\n",
    "SLEEP_MIN = 6.5\n",
    "SLEEP_MAX = 12.0\n",
    "\n",
    "# =========================\n",
    "# Session to√†n c·ª•c\n",
    "# =========================\n",
    "SESS = creq.Session(\n",
    "    headers=COMMON_HEADERS,\n",
    "    timeout=30,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HTTP helper\n",
    "# =========================\n",
    "def http_get(url: str, max_retries: int = 4) -> Optional[creq.Response]:\n",
    "    \"\"\"\n",
    "    GET v·ªõi retry/backoff. ƒê·ªïi impersonate n·∫øu 403/429.\n",
    "    Tr·∫£ v·ªÅ Response 200 ho·∫∑c None n·∫øu th·∫•t b·∫°i.\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    for i in range(max_retries):\n",
    "        imp = IMP_CHOICES[min(i, len(IMP_CHOICES) - 1)]\n",
    "        try:\n",
    "            r = SESS.get(url, impersonate=imp, allow_redirects=True)\n",
    "            code = r.status_code\n",
    "            if code == 200:\n",
    "                return r\n",
    "            if code in (403, 429):\n",
    "                # B·ªã ch·∫∑n / rate limit ‚Üí backoff + jitter r·ªìi th·ª≠ ti·∫øp\n",
    "                time.sleep(delay + random.uniform(0.5, 1.8))\n",
    "                delay *= 1.8\n",
    "                continue\n",
    "            # L·ªói kh√°c: th·ª≠ 1-2 l·∫ßn nh·∫π\n",
    "            time.sleep(1.0)\n",
    "        except Exception:\n",
    "            time.sleep(delay)\n",
    "            delay *= 1.8\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Parsers\n",
    "# =========================\n",
    "def extract_review_data(soup: BeautifulSoup) -> dict:\n",
    "    container = soup.find(\"div\", class_=\"profile-review__data\")\n",
    "    if not container:\n",
    "        return {}\n",
    "\n",
    "    result: dict = {}\n",
    "    li_items = container.select(\"ul.data--list > li.data--item\")\n",
    "\n",
    "    for li in li_items:\n",
    "        tooltip_html = li.get(\"data-tooltip-content\", \"\")\n",
    "        tooltip_label = (\n",
    "            BeautifulSoup(tooltip_html, \"html.parser\").get_text(strip=True)\n",
    "            if tooltip_html\n",
    "            else None\n",
    "        )\n",
    "        text_parts = [\n",
    "            t.strip()\n",
    "            for t in li.stripped_strings\n",
    "            if not t.lower().startswith(\"show more\")\n",
    "        ]\n",
    "        if not text_parts:\n",
    "            continue\n",
    "\n",
    "        if tooltip_label:\n",
    "            result[tooltip_label] = \" \".join(text_parts)\n",
    "        else:\n",
    "            result.setdefault(\"unknown\", []).append(\" \".join(text_parts))\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_reviewer_info(soup: BeautifulSoup) -> dict:\n",
    "    container = soup.find(\"div\", class_=\"profile-review__reviewer\")\n",
    "    if not container:\n",
    "        return {}\n",
    "\n",
    "    result: dict = {}\n",
    "\n",
    "    # T√™n reviewer\n",
    "    name_tag = container.find(\"div\", class_=\"reviewer_card--name\")\n",
    "    if name_tag:\n",
    "        result[\"reviewer_name\"] = name_tag.get_text(strip=True)\n",
    "\n",
    "    # Ch·ª©c v·ª• & c√¥ng ty\n",
    "    position_tag = container.find(\"div\", class_=\"reviewer_position\")\n",
    "    if position_tag:\n",
    "        text = position_tag.get_text(strip=True)\n",
    "        result[\"reviewer_position_raw\"] = text\n",
    "        if \",\" in text:\n",
    "            parts = [p.strip() for p in text.split(\",\", 1)]\n",
    "            result[\"reviewer_role\"] = parts[0]\n",
    "            result[\"reviewer_company\"] = parts[1]\n",
    "        else:\n",
    "            result[\"reviewer_role\"] = text\n",
    "\n",
    "    # Tr·∫°ng th√°i verified\n",
    "    verified_tag = container.find(\n",
    "        \"span\", class_=\"profile-review__reviewer-verification-badge-title\"\n",
    "    )\n",
    "    if verified_tag:\n",
    "        result[\"verified_status\"] = verified_tag.get_text(strip=True)\n",
    "\n",
    "    # Industry, Location, Client size, Review type\n",
    "    list_items = container.select(\"ul.reviewer_list > li.reviewer_list--item\")\n",
    "    for li in list_items:\n",
    "        tooltip_html = li.get(\"data-tooltip-content\", \"\")\n",
    "        label = (\n",
    "            BeautifulSoup(tooltip_html, \"html.parser\").get_text(strip=True)\n",
    "            if tooltip_html\n",
    "            else None\n",
    "        )\n",
    "        value_tag = li.find(\"span\", class_=\"reviewer_list__details-title\")\n",
    "        value = value_tag.get_text(strip=True) if value_tag else None\n",
    "        if label and value:\n",
    "            result[label] = value\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_social_links(scope: BeautifulSoup) -> dict:\n",
    "    \"\"\"\n",
    "    L·∫•y social links ·ªü v√πng 'scope' (th∆∞·ªùng l√† section#contact; n·∫øu kh√¥ng c√≥ th√¨ truy·ªÅn c·∫£ trang).\n",
    "    \"\"\"\n",
    "    links: dict = {}\n",
    "    social_links = scope.select(\n",
    "        \"div.profile-social-media__wrap a.profile-social-media__link\"\n",
    "    )\n",
    "    for a in social_links:\n",
    "        label = a.get(\"data-type\") or a.get_text(strip=True)\n",
    "        href = a.get(\"href\")\n",
    "        if label and href:\n",
    "            links[f\"{label.lower()} Company Outsource\"] = href\n",
    "    return links\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Scrapers\n",
    "# =========================\n",
    "def get_base_url_company(start_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    L·∫•y danh s√°ch URL company profile t·ª´ trang directory.\n",
    "    \"\"\"\n",
    "    ans: List[str] = []\n",
    "    resp = http_get(start_url)\n",
    "    if not resp:\n",
    "        print(\"‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c trang list (403/timeout).\", file=sys.stderr)\n",
    "        return ans\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    providers_list = soup.find(id=\"providers__list\")\n",
    "    if not providers_list:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y providers__list (c√≥ th·ªÉ b·ªã anti-bot).\", file=sys.stderr)\n",
    "        return ans\n",
    "\n",
    "    for li in providers_list.find_all(\"li\", class_=\"provider-list-item\"):\n",
    "        cta = li.find(\"div\", class_=\"provider__cta-container\")\n",
    "        if not cta:\n",
    "            continue\n",
    "        a = cta.find(\n",
    "            \"a\",\n",
    "            class_=\"provider__cta-link sg-button-v2 sg-button-v2--secondary directory_profile\",\n",
    "        )\n",
    "        if a and a.get(\"href\"):\n",
    "            ans.append(urljoin(\"https://clutch.co\", a[\"href\"]))\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_detail_information(company_url: str) -> tuple[str,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    L·∫•y b·∫£ng reviews + social links t·ª´ 1 trang company.\n",
    "    \"\"\"\n",
    "    resp = http_get(company_url)\n",
    "    if not resp:\n",
    "        print(f\"‚ö†Ô∏è B·ªè qua (403/timeout): {company_url}\", file=sys.stderr)\n",
    "        return \"TIME_OUT\",pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    contact_scope = soup.find(\"section\", id=\"contact\") or soup  # fallback\n",
    "    link_social = extract_social_links(contact_scope)\n",
    "\n",
    "    reviews_wrap = soup.find(\"div\", class_=\"profile-reviews--list__wrapper\")\n",
    "    if not reviews_wrap:\n",
    "        print(f\"‚ÑπÔ∏è Kh√¥ng th·∫•y reviews: {company_url}\")\n",
    "        return \"NO_REVIEW\",pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    elements = reviews_wrap.find_all(\"article\", class_=\"profile-review\")\n",
    "    print(f\"üîç {company_url} ‚Üí {len(elements)} reviews\")\n",
    "\n",
    "    for e in elements:\n",
    "        project_data = extract_review_data(e)\n",
    "        desc_el = e.find(\"div\", class_=\"profile-review__summary mobile_hide\")\n",
    "        viewmore_data = e.find(\"div\", class_=\"profile-review__extra mobile_hide desktop-review-to-hide hidden\")\n",
    "        background_text = \"\"\n",
    "        if viewmore_data:\n",
    "            background_el = viewmore_data.find(\"div\", class_=\"profile-review__text with-border profile-review__extra-section\")\n",
    "            background_text = background_el.get_text(strip=True) if background_el else \"\"\n",
    "        reviewer_data = extract_reviewer_info(e)\n",
    "\n",
    "        row: dict = {}\n",
    "        row.update(project_data or {})\n",
    "        row.update(reviewer_data or {})\n",
    "        row[\"Project description\"] = desc_el.get_text(strip=True) if desc_el else None\n",
    "        row[\"background\"] = background_text\n",
    "        row.update(link_social or {})\n",
    "        # print('ROW :', row)\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return \"NO_REVIEW\", df\n",
    "\n",
    "    preferred_cols = [\n",
    "        \"reviewer_name\",\n",
    "        \"reviewer_role\",\n",
    "        \"reviewer_company\",\n",
    "        \"verified_status\",\n",
    "        \"Industry\",\n",
    "        \"Location\",\n",
    "        \"Client size\",\n",
    "        \"Review type\",\n",
    "        \"Services\",\n",
    "        \"Project size\",\n",
    "        \"Project length\",\n",
    "        \"Project description\",\n",
    "        \"background\"\n",
    "    ]\n",
    "    cols = [c for c in preferred_cols if c in df.columns] + [\n",
    "        c for c in df.columns if c not in preferred_cols\n",
    "    ]\n",
    "    return \"HAVE_REVIEW\",df.loc[:, cols]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Clutch.co crawler\")\n",
    "    p.add_argument(\n",
    "        \"--start-url\",\n",
    "        type=str,\n",
    "        default=DEFAULT_START_URL,\n",
    "        help=f\"Directory URL b·∫Øt ƒë·∫ßu (default: {DEFAULT_START_URL})\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--limit\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"S·ªë company t·ªëi ƒëa ƒë·ªÉ crawl (default: 20)\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--out\",\n",
    "        type=str,\n",
    "        default=\"clutch_reviews.csv\",\n",
    "        help=\"ƒê∆∞·ªùng d·∫´n file CSV xu·∫•t ra (default: clutch_reviews.csv)\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--checkpoint',\n",
    "        type=str,\n",
    "        default='checkpoint.json',\n",
    "        help='File checkpoint ƒë·ªÉ l∆∞u tr·∫°ng th√°i crawl (default: checkpoint.json)',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--workers',\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help='S·ªë worker threads ƒë·ªÉ crawl ƒë·ªìng th·ªùi (default: 8)',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--flush-every',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Flush k·∫øt qu·∫£ ra file sau m·ªói N company (default: 20)',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--last_page',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Trang cu·ªëi c√πng crawl (default: 1)',\n",
    "    )\n",
    "    return p.parse_args()   \n",
    "\n",
    "SLEEP_MIN, SLEEP_MAX = 0.5, 1.5  \n",
    "\n",
    "# ==== Helper: atomic write CSV ====\n",
    "def atomic_write_csv(df: pd.DataFrame, path: str, header: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Append-only, kh√¥ng bao gi·ªù replace file hi·ªán c√≥.\n",
    "    Ghi header ch·ªâ khi file tr·ªëng.\n",
    "    C√≥ kh√≥a file ƒë·ªÉ tr√°nh ghi ƒë√® khi multi-thread.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "\n",
    "    # M·ªü file ·ªü ch·∫ø ƒë·ªô append-binary; t·ª± t·∫°o n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"ab+\") as fout:\n",
    "        # Kh√≥a ƒë·ªôc quy·ªÅn\n",
    "        fcntl.flock(fout, fcntl.LOCK_EX)\n",
    "\n",
    "        # Ki·ªÉm tra k√≠ch th∆∞·ªõc sau khi ƒë√£ kh√≥a (tr√°nh race)\n",
    "        fout.seek(0, os.SEEK_END)\n",
    "        is_empty = (fout.tell() == 0)\n",
    "\n",
    "        # Ch·ªâ ghi header khi file tr·ªëng v√† caller cho ph√©p header\n",
    "        write_header = header and is_empty\n",
    "\n",
    "        # Ghi ra file t·∫°m\n",
    "        df.to_csv(tmp, index=False, header=write_header)\n",
    "\n",
    "        # Append n·ªôi dung tmp v√†o cu·ªëi file ƒë√≠ch\n",
    "        with open(tmp, \"rb\") as fin:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "\n",
    "        # M·ªü kh√≥a & d·ªçn file t·∫°m\n",
    "        fcntl.flock(fout, fcntl.LOCK_UN)\n",
    "    tmp.unlink(missing_ok=True)\n",
    "# ==== Checkpoint ====\n",
    "def load_checkpoint(ckpt_file: str) -> Dict[str, Any]:\n",
    "    p = Path(ckpt_file)\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return {\"done_urls\": [], \"last_page\": 1}\n",
    "    return {\"done_urls\": [], \"last_page\": 1}\n",
    "\n",
    "def save_checkpoint(ckpt_file: str, state: Dict[str, Any]) -> None:\n",
    "    p = Path(ckpt_file)\n",
    "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
    "    tmp.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    tmp.replace(p)\n",
    "\n",
    "# ==== Retry wrapper cho get_detail_information ====\n",
    "def get_detail_information_with_retry(url: str, max_retry: int = 3, backoff_base: float = 0.8) -> tuple[str, pd.DataFrame]:\n",
    "    \n",
    "    for attempt in range(1, max_retry + 1):\n",
    "        try:\n",
    "            status, df = get_detail_information(url)  # <- d√πng h√†m g·ªëc c·ªßa b·∫°n\n",
    "            if df is None:\n",
    "                df = pd.DataFrame()\n",
    "            return status, df\n",
    "        except Exception as e:\n",
    "            if attempt == max_retry:\n",
    "                return \"ERROR\", pd.DataFrame()\n",
    "            sleep_s = (backoff_base ** attempt) + random.uniform(0.05, 0.25)\n",
    "            time.sleep(sleep_s)\n",
    "    return \"TIME_OUT\", pd.DataFrame()\n",
    "\n",
    "def process_company(url: str) -> Tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ (train_df, test_df, url) cho 1 company.\n",
    "    \"\"\"\n",
    "    train_parts: List[pd.DataFrame] = []\n",
    "    test_parts: List[pd.DataFrame] = []\n",
    "\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            url_review = url\n",
    "        else:\n",
    "            url_review = f\"{url}?page={i}#reviews\"\n",
    "\n",
    "        status, df = get_detail_information_with_retry(url_review)\n",
    "        if status in (\"TIME_OUT\", \"ERROR\", \"NO_REVIEW\"):\n",
    "            train_df = pd.concat(train_parts, ignore_index=True) if train_parts else pd.DataFrame()\n",
    "            test_df  = pd.concat(test_parts,  ignore_index=True) if test_parts  else pd.DataFrame()\n",
    "            return train_df, test_df, url\n",
    "        if df is not None and not df.empty and len(df) > 5:\n",
    "            tr, te = train_test_split(df, test_size=0.3, shuffle=True, random_state=42)\n",
    "            train_parts.append(tr)\n",
    "            test_parts.append(te)\n",
    "\n",
    "        time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))\n",
    "\n",
    "    train_df = pd.concat(train_parts, ignore_index=True) if train_parts else pd.DataFrame()\n",
    "    test_df  = pd.concat(test_parts,  ignore_index=True) if test_parts  else pd.DataFrame()\n",
    "    return train_df, test_df, url\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    out_path = args.out\n",
    "    test_out_path = f\"{out_path.replace('.csv', '_test.csv')}\"\n",
    "    ckpt_file = getattr(args, \"checkpoint\", \"checkpoint.json\")\n",
    "    workers = max(1, getattr(args, \"workers\", 8))\n",
    "    flush_every = max(1, getattr(args, \"flush_every\", 20)) \n",
    "\n",
    "    state = load_checkpoint(ckpt_file)\n",
    "    done_urls = set(state.get(\"done_urls\", []))\n",
    "    last_page = int(state.get(\"last_page\", 1))\n",
    "\n",
    "    for com in range(last_page, 100):\n",
    "        start_url = args.start_url\n",
    "        # (G·ª£i √Ω) C√≥ th·ªÉ mu·ªën set page lu√¥n (kh√¥ng ch·ªâ com==1) t√πy site:\n",
    "        if com > 1:\n",
    "            start_url = f\"{start_url}?page={com}\"\n",
    "\n",
    "        company_urls = get_base_url_company(start_url)\n",
    "        if not company_urls:\n",
    "            print(\"‚ö†Ô∏è Kh√¥ng c√≥ URL c√¥ng ty n√†o ƒë∆∞·ª£c t√¨m th·∫•y. K·∫øt th√∫c.\", file=sys.stderr)\n",
    "            state[\"last_page\"] = com + 1\n",
    "            save_checkpoint(ckpt_file, state)\n",
    "            return\n",
    "\n",
    "        company_urls = [u for u in company_urls if u not in done_urls]\n",
    "        print(\"=========================>\")\n",
    "        print(f\"üîé Trang {com}: c√≤n {len(company_urls)} c√¥ng ty c·∫ßn crawl (t·ªïng trang c√≥ th·ªÉ l·ªõn h∆°n).\")\n",
    "\n",
    "        batch_trains: List[pd.DataFrame] = []\n",
    "        batch_tests: List[pd.DataFrame] = []\n",
    "        futures = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            for url in company_urls:\n",
    "                futures.append(ex.submit(process_company, url))\n",
    "\n",
    "            # Thu k·∫øt qu·∫£ theo t·ª´ng company, flush m·ªói 'flush_every'\n",
    "            pbar = tqdm(as_completed(futures), total=len(futures), desc=f\"Page {com}\")\n",
    "            for fut in pbar:\n",
    "                tr_df, te_df, url = fut.result()\n",
    "                if tr_df is not None and not tr_df.empty:\n",
    "                    batch_trains.append(tr_df)\n",
    "                if te_df is not None and not te_df.empty:\n",
    "                    batch_tests.append(te_df)\n",
    "                done_urls.add(url)\n",
    "                # print('URL done:', url)\n",
    "                # print('TR_df :',len(tr_df), 'TE_df :', len(te_df))\n",
    "                # print('Batch trains:', len(batch_trains), 'Batch tests:', len(batch_tests))\n",
    "                # print('Flush every:', flush_every)\n",
    "                if len(batch_trains) >= flush_every or len(batch_tests) >= flush_every:\n",
    "                    if batch_trains:\n",
    "                        big_tr = pd.concat(batch_trains, ignore_index=True)\n",
    "                        atomic_write_csv(big_tr, out_path,  header=True)\n",
    "                        batch_trains.clear()\n",
    "                    if batch_tests:\n",
    "                        big_te = pd.concat(batch_tests, ignore_index=True)\n",
    "                        atomic_write_csv(big_te, test_out_path, header=True)\n",
    "                        batch_tests.clear()\n",
    "\n",
    "                    state[\"done_urls\"] = list(done_urls)\n",
    "                    state[\"last_page\"] = com\n",
    "                    save_checkpoint(ckpt_file, state)\n",
    "\n",
    "        if batch_trains:\n",
    "            big_tr = pd.concat(batch_trains, ignore_index=True)\n",
    "            atomic_write_csv(big_tr, out_path,  header=True)\n",
    "        if batch_tests:\n",
    "            big_te = pd.concat(batch_tests, ignore_index=True)\n",
    "            atomic_write_csv(big_te, test_out_path, header=True)\n",
    "\n",
    "        state[\"done_urls\"] = list(done_urls)\n",
    "        state[\"last_page\"] = com + 1\n",
    "        save_checkpoint(ckpt_file, state)\n",
    "\n",
    "    print(f\"‚úÖ Ho√†n t·∫•t. K·∫øt qu·∫£ ·ªü: {out_path} v√† {test_out_path}\")\n",
    "company_urls = get_base_url_company(f\"{DEFAULT_START_URL}?page=410\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9fa4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://clutch.co/profile/yantra',\n",
       " 'https://clutch.co/profile/hostedpci',\n",
       " 'https://clutch.co/profile/synergy-technologies',\n",
       " 'https://clutch.co/profile/emagined-security',\n",
       " 'https://clutch.co/profile/eo-security-sro',\n",
       " 'https://clutch.co/profile/soil-institute-management',\n",
       " 'https://clutch.co/profile/appsecure-security',\n",
       " 'https://clutch.co/profile/salesbabu-business-solutions-0',\n",
       " 'https://clutch.co/profile/ctc-technologies',\n",
       " 'https://clutch.co/profile/twintel-solutions',\n",
       " 'https://clutch.co/profile/relevance-management',\n",
       " 'https://clutch.co/profile/acecloud-rtds',\n",
       " 'https://clutch.co/profile/threadgold-consulting',\n",
       " 'https://clutch.co/profile/belue-creative',\n",
       " 'https://clutch.co/profile/sprocket-security',\n",
       " 'https://clutch.co/profile/gac-business-solutions',\n",
       " 'https://clutch.co/profile/axxis-consulting',\n",
       " 'https://clutch.co/profile/konsulko-group',\n",
       " 'https://clutch.co/profile/isa-cybersecurity',\n",
       " 'https://clutch.co/profile/powersoft-computer-solutions',\n",
       " 'https://clutch.co/profile/coosy',\n",
       " 'https://clutch.co/profile/effiasoft',\n",
       " 'https://clutch.co/profile/lia-digital',\n",
       " 'https://clutch.co/profile/cpx',\n",
       " 'https://clutch.co/profile/inscale',\n",
       " 'https://clutch.co/profile/topline-results',\n",
       " 'https://clutch.co/profile/sfappsinfo',\n",
       " 'https://clutch.co/profile/rkdewan',\n",
       " 'https://clutch.co/profile/ridge-security',\n",
       " 'https://clutch.co/profile/kl-software-technologies',\n",
       " 'https://clutch.co/profile/powertrain',\n",
       " 'https://clutch.co/profile/praxis-info-solutions',\n",
       " 'https://clutch.co/profile/atwnet',\n",
       " 'https://clutch.co/profile/awareson',\n",
       " 'https://clutch.co/profile/sectricity',\n",
       " 'https://clutch.co/profile/sellmore-gmbh',\n",
       " 'https://clutch.co/profile/rospand-global-techno-service-private',\n",
       " 'https://clutch.co/profile/profex-tech',\n",
       " 'https://clutch.co/profile/secureleap',\n",
       " 'https://clutch.co/profile/ribbitz',\n",
       " 'https://clutch.co/profile/pathways-digital',\n",
       " 'https://clutch.co/profile/reachout',\n",
       " 'https://clutch.co/profile/chordify-management-consultants',\n",
       " 'https://clutch.co/profile/swift-future',\n",
       " 'https://clutch.co/profile/pancentric-digital',\n",
       " 'https://clutch.co/profile/icorp-0',\n",
       " 'https://clutch.co/profile/dashbird',\n",
       " 'https://clutch.co/profile/skubotics',\n",
       " 'https://clutch.co/profile/exergy-systems',\n",
       " 'https://clutch.co/profile/avs-cyber']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
